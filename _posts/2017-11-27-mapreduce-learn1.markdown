---
layout: post
title:  "大数据技术原理与应用之MapReduce(一)"
date:   2017-11-27 16:00:00 +0800
tag: MapReduce
category: 2017年11月
description: "大数据技术原理与应用之MapReduce(一),包括概述和工作流程"
keywords: MapReduce,Map,Reduce,Hadoop
type: coding
user: kevis
---

### MapReduce 概述
　　MapReduce是一种并行编程模型，用于大规模数据集（大于1TB）的并行运算，它将复杂的，运行于大规模集群上的并行计算过程高度抽象到两个函数：Map和Reduce，这两个函数及其核心思想都源自函数式编程语言。

　　在MapReduce中，一个存储在分布式文件系统中的大规模数据集会被切分成许多独立的小数据块，这些小数据块可以被多个Map任务并行处理。MapReduce框架会为每个Map任务输入一个小数据子集，Map任务生成的结果会继续作为Reduce任务的输入，最终由Reduce任务输出最后结果，并写入分布式文件系统。<strong>适合用MapReduce来处理的数据集需要满足一个前提条件：待处理的数据集可以分解成许多小的数据集，而且每一个小数据集都可以完全并行地进行处理。 </strong>

　　MapReduce设计的一个理念就是“计算向数据靠拢”，而不是“数据向计算靠拢”，因为移动数据需要大量的网络传输开销，尤其是在大规模数据环境下，这种开销尤为惊人，所以<strong>移动计算要比移动数据更加经济</strong>。本着这个理念，在一个集群中，只要有可能，MapReduce框架就会将Map程序就近地在HDFS数据所在的节点运行，即将计算节点和存储节点放在一起运行，从而减少了节点间的数据移动开销。  

　　MapReduce模型的核心是Map函数和Reduce函数，二者都是由应用程序开发者负责具体实现的。MapReduce编程之所以比较容易，是因为我们只需要关注如何实现Map和Reduce函数，而不需要处理并行编程中的其他各种复杂问题，如分布式存储、工作调度、负载均衡、容错处理、网络通信等，这些问题都会由MapReduce框架负责处理。

　　Map函数和Reduce函数都是以<key,value>作为输入，按一定的映射规则转换成另一个或一批<key,value>进行输出。


函数 | 输入 | 输出 | 说明
---------|---------|---------|--------------
Map  | <k1,v1> | List(<k2,v2>) |  (1)将小数据集进一步解析成一批<key,value>对，输入Map函数中进行处理  (2)每一个输入的<k1,v1>会输出一批<k2,v2>，<k2,v2>是计算的中间结果
Reduce | <k2,List(v2)> | <k3,v3> | 输入的中间结果<k2,List(v2)>中的List(v2)表示是一批属于同一个k2的value

<br>
　　Map函数的输入来自于分布式文件系统的文件块，这些文件块的格式是任意的，可以是文档，也可以是二进制格式的。文件块是一系列元素的集合，这些元素也是任意类型的，同一个元素不能跨文件块存储。Map函数将输入的元素转换成<key,value>形式的键值对，键和值的类型也是任意的，其中键不同于一般的标志属性，即键没有唯一性，不能作为输出的身份标识，即使是同一个输入元素，也可通过一个Map任务生成具有相同键的多个<key,value>。  

　　Reduce函数的任务就是将输入的一系列具有相同键的键值对以某种方式组合起来，输出处理后的键值对，输出结果会合并成一个文件。用户可以指定Reduce任务的个数（如n个），并通知实现系统，然后主控进程通常会选择一个Hash函数，Map任务输出的每个键都会经过Hash函数计算，并根据哈希结果将该键值对输入相应的Reduce任务来处理。对于处理键为k的Reduce任务的输入形式为<k,<v1,v2,...,vn>>,输出为<k,V>。  

### MapReduce的工作流程
#### 工作流程概述

　　大规模数据集的处理包括分布式存储和分布式计算两个核心环节。  
　　MapReduce的输入和输出都需要借助于分布式文件系统进行存储，这些文件被分布存储到集群中的多个节点上。MapReduce的核心思想可以用“分而治之”来描述，也就是把一个大的数据集拆分成多个小数据块在多台机器上并行处理，也就是说，一个大的MapReduce作业，首先会被拆分成许多个Map任务在多台机器上并行执行，每个Map任务通常运行在数据存储的节点上，这样，计算和数据就可以放在一起运行，不需要额外的数据传输开销。当Map任务结束后，会生成以<key,value>形式表示的许多中间结果。然后，这些中间结果会被分发到多个Reduce任务在多台机器上并行执行，具有相同key的<key,value>会被发送到同一个Reduce任务那里，Reduce任务会对中间结果进行汇总计算得到最后结果，并输出到分布式文件系统中。
　　需要指出的是，不同的Map任务之间不会进行通信，不同的是Reduce任务之间也不会发生任何信息交换；用户不能显式地从一台机器向另一台机器发送消息，所有的数据交换都是通过MapReduce框架自身去实现的。
　　在MapReduce的整个执行过程中，Map任务的输入文件、Reduce任务的处理结果都是保存在分布式文件系统中的，而Map任务处理得到的中间结果则保存在本地存储中（如磁盘）。另外，只有当Map处理全部结束后，Reduce过程才能开始；只有Map需要考虑数据局部性，实现“计算向数据靠拢”，而Reduce则无需考虑数据局部性。  

#### MapReduce的各个执行阶段

##### 一个MapReduce算法的执行过程：  

```
    (1) MapReduce框架使用InputFormat模块做Map前的预处理，比如验证输入的格式是否符合输入定义；然后，将输入文件切分为逻辑上的多个InputSplit,InputSplit是MapReduce对文件进行处理和运算的输入单位，只是一个逻辑概念，每个InputSplit并没有对文件进行实际切割，只是记录了要处理的数据的位置和长度。
    (2) 因为InputSplit是逻辑切分而非物理切分，所以还需要通过RecordReader(RR)根据InputSplit中的信息来处理InputSplit中的具体记录，加载数据并转换为适合Map任务读取的键值对，输入给Map任务。
    (3) Map任务会根据用户自定义的映射规则，输出一系列的<key,value>作为中间结果。
    (4) 为了让Reduce可以并行处理Map的结果，需要对Map的输出进行一定的分区(Portition)、排序(Sort)、合并(Combine)、归并(Merge)等操作，得到<key,value>到有序的<key,value-list>，这个过程用Shuffle(洗牌)来称呼是非常形象的。
    (5) Reduce以一系列<key,value-list>中间结果作为输入，执行用户定义的逻辑，输出结果给OutputFormat模块。
    (6) OutputFormat模块会验证输出目录是否已经存在以及输出结果类型是否符合配置文件中的配置类型，如果都满足，就输出Reduce的结果到分布式文件系统。
```

#### Shuffle过程详解
　　Shuffle过程是MapReduce整个工作流程的核心环节，理解Shuffle过程的基本原理，对于理解MapReduce流程至关重要。
##### Shuffle过程简介
　　所谓Shuffle，是指对Map输出结果进行分区、排序、合并等处理并交给Reduce的过程。因此，Shuffle过程分为Map端的操作和Reduce端的操作。    
　　(1) 在Map端的Shuffle过程  
　　Map的输出结果首先被写入缓存，当缓存满时，就启动溢写操作，把缓存中的数据写入磁盘文件，并清空缓存。当启动溢写操作时，首先需要把缓存中的数据进行分区，然后对每个分区的数据进行排序（Sort）和合并（Combine），之后再写入磁盘文件。每次溢写操作会生成一个新的磁盘文件，随着Map任务的执行，磁盘中就会生成多个溢写文件。在Map任务全部结束之前，这些溢写文件会被归并（Merge）成一个大的磁盘文件，然后通知相应的Reduce任务来领取属于自己处理的数据集。  
　　(2) 在Reduce端的Shuffle过程  
　　Reduce任务从Map端的不同Map机器领回属于自己处理的那部分数据，然后对数据进行归并（Merge）后交给Reduce处理。
##### Map端的Shuffle过程
　　Map端的Shuffle过程包括4个步骤：  
　　(1) 输入数据和执行Map任务  
　　Map任务的输入数据一般保存在分布式文件系统（GFS或HDFS）的文件块中，这些文件块的格式是任意的，可以是文档，也可以是二进制格式的。Map任务接受<key,value>作为输入后，按一定映射规则转换成一批<key,value>进行输出。  
　　(2) 写入缓存  
　　每个Map任务都会被分配一个缓存，Map的输出结果不是立即写入磁盘，而是首先写入缓存。在缓存中积累一定数量的Map输出结果以后，再一次性批量写入磁盘，这样可以大大减少对磁盘I/O的影响。因为，磁盘包含机械部件，它是通过磁头移动和盘片的转动来寻址定位数据的，每次寻址的开销很大，如果每个Map输出结果都直接写入磁盘，会引入很多次寻址开销，而一次性批量写入，就只需要一次寻址，连续写入，大大降低了开销。需要注意的是，在写入缓存之前，key与value值都会被序列化成字节数组。  
　　(3) 溢写（分区、排序和合并）  
　　提供给MapReduce的缓存的容量是有限的，默认大小是100MB。随着Map任务的执行，缓存中Map结果的数量会不断增加，很快就会占满整个缓存。这时，就必须启动溢写（Spill）操作，把缓存中的内容一次性写入磁盘，并清空缓存。溢写的过程通常是由另外一个单独的后台线程来完成的，不会影响Map结果往缓存写入，但是为了保证Map结果能够不停地持续写入缓存，不受溢写过程的影响，就必须让缓存中一直有可用的空间，不能等到全部占满才启动溢写过程，所以一般会设置一个溢写比例，如0.8，也就是说，当100MB大小的缓存被填满80MB数据时，就启动溢写过程，把已经写入的80MB数据写入磁盘，剩余20MB空间供Map结果继续写入。  
　　但是，在溢写到磁盘之前，缓存中的数据首先会被分区（Partition）。缓存中的数据是<key,value>形式的键值对，这些键值对最终需要交给不同的Reduce任务进行并行处理。MapReduce通过Partitioner接口对这些键值对进行分区，默认采用的分区方式是采用Hash函数对key进行哈希后再用Reduce任务的数量进行取模，可以表示成hash(key) mod R,其中R表示Reduce任务的数量，这样，就可以把Map输出结果均匀地分配给这R个Reduce任务去并行处理了。当然，MapReduce也允许用户通过重载Partitioner接口来自定义分区方式。  
　　对于每个分区内的所有键值对，后台线程会根据key对它们进行内存排序（Sort），排序是MapReduce的默认操作。排序结束后，还包含一个可选的合并（Combine）操作。如果用户事先没有定义Combiner函数，就不用进行合并操作。如果用户事先定义了Combiner函数，则这个时候会执行合并操作，从而减少需要溢写到磁盘的数据量。  
　　所谓“合并”，是指将那些具有相同key的<key,value>的value加起来。比如，有两个键值对<"xmu" 1>和<"xmu" 1>，经过合并操作以后就可以得到一个键值对<"xmu" 2>，减少了键值对的数量。这里需要注意，Map端的这种合并操作，其实和Reduce的功能相似，但是由于这个操作发生在Map端，所以我们只能称之为“合并”，从而有别于Reduce。不过，并非所有场合都可以使用Combiner，因为Combiner的输出是Reduce任务的输入，Combiner绝不能改变Reduce任务最终的计算结果，一般而言，累加、最大值等场景可以使用合并操作。  
　　经过分区、排序以及可能发生的合并操作之后，这些缓存中的键值对就可以被写入磁盘，并清空缓存。每次溢写操作都会在磁盘中生成一个新的溢写文件，写入溢写文件中的所有键值对都是经过分区和排序的。  
　　(4)文件归并  
　　每次溢写操作都会在磁盘中生成一个新的溢写文件，随着MapReduce任务的进行，磁盘中的溢写文件数量会越来越多。当然，如果Map输出结果很少，磁盘上只会存在一个溢写文件，但是通常都会存在多个溢写文件。最终，在Map任务全部结束之前，系统会对所有溢写文件中的数据进行归并（Merge），生成一个大的溢写文件，这个大的溢写文件中的所有键值对也是经过分区和排序的。  
　　









